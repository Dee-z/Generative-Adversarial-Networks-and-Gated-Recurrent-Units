{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sst52/Neural-Nets/blob/master/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KRX_s-H9BWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "\n",
        "\n",
        "def floatX(X):\n",
        "    return np.asarray(X, dtype=theano.config.floatX)\n",
        "\n",
        "def norm_weight(shape, scale=0.01):\n",
        "    return scale * numpy.random.randn(shape)\n",
        "\n",
        "def ortho_weight(n):\n",
        "    W = np.random.randn(n, n)\n",
        "    u, s, v = np.linalg.svd(W)\n",
        "    return u\n",
        "\n",
        "def xavier_weight(shape):\n",
        "    return np.random.uniform(-np.sqrt(6. / (shape[0] + shape[1])), np.sqrt(6. / (shape[0] + shape[1])), shape)\n",
        "\n",
        "def init_weights(shape, name, sample = \"xavier\", scale = 0.01):\n",
        "    if sample == \"norm\":\n",
        "        values = norm_weight(shape, scale)\n",
        "    elif sample == \"xavier\":\n",
        "        values = xavier_weight(shape)\n",
        "    elif sample == \"ortho\":\n",
        "        values = ortho_weight(shape[0])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported initialization scheme: %s\" % sample)\n",
        "    return theano.shared(floatX(values), name)\n",
        "\n",
        "def init_weights_2(shape, name, sample = \"xavier\", scale = 0.01, couple_axis = 1):\n",
        "    if couple_axis in [0, 1]:\n",
        "        if sample == \"norm\":\n",
        "            values = np.concatenate([norm_weight(shape, scale),\n",
        "                                     norm_weight(shape, scale)], couple_axis)\n",
        "        elif sample == \"xavier\":\n",
        "            values = np.concatenate([xavier_weight(shape),\n",
        "                                     xavier_weight(shape)], couple_axis)\n",
        "        elif sample == \"ortho\":\n",
        "            values = np.concatenate([ortho_weight(shape[0]),\n",
        "                                     ortho_weight(shape[0])], couple_axis)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported initialization scheme: %s\" % sample)\n",
        "    return theano.shared(floatX(values), name)\n",
        "\n",
        "def init_weights_4(shape, name, sample = \"xavier\", scale = 0.01, couple_axis = 1):\n",
        "    if couple_axis in [0, 1]:\n",
        "        if sample == \"norm\":\n",
        "            values = np.concatenate([norm_weight(shape, scale),\n",
        "                                     norm_weight(shape, scale),\n",
        "                                     norm_weight(shape, scale),\n",
        "                                     norm_weight(shape, scale)], couple_axis)\n",
        "        elif sample == \"xavier\":\n",
        "            values = np.concatenate([xavier_weight(shape),\n",
        "                                     xavier_weight(shape),\n",
        "                                     xavier_weight(shape),\n",
        "                                     xavier_weight(shape)], couple_axis)\n",
        "        elif sample == \"ortho\":\n",
        "            values = np.concatenate([ortho_weight(shape[0]),\n",
        "                                     ortho_weight(shape[0]),\n",
        "                                     ortho_weight(shape[0]),\n",
        "                                     ortho_weight(shape[0])], couple_axis)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported initialization scheme: %s\" % sample)\n",
        "    return theano.shared(floatX(values), name)\n",
        "\n",
        "def init_gradws(shape, name):\n",
        "    return theano.shared(floatX(np.zeros(shape)), name)\n",
        "\n",
        "def init_bias(size, name):\n",
        "    return theano.shared(floatX(np.zeros((size,))), name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpAVdD6R8dLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import theano\n",
        "import theano.tensor as T\n",
        "\n",
        "class GRULayer(object):\n",
        "    def __init__(self, rng, layer_id, shape, X, mask, is_train = 1, batch_size = 1, p = 0.5):\n",
        "        prefix = \"GRU_\"\n",
        "        layer_id = \"_\" + layer_id\n",
        "        self.in_size, self.out_size = shape\n",
        "        \n",
        "        self.W_x_rz = init_weights_2((self.in_size, self.out_size), prefix + \"W_x_rz\" + layer_id, sample = \"xavier\")\n",
        "        self.W_h_rz = init_weights_2((self.out_size, self.out_size), prefix + \"W_h_rz\" + layer_id, sample = \"ortho\")\n",
        "        self.b_rz = init_bias(self.out_size * 2, prefix + \"b_rz\" + layer_id)\n",
        "        \n",
        "        self.W_xh = init_weights((self.in_size, self.out_size), prefix + \"W_xh\" + layer_id, sample = \"xavier\")\n",
        "        self.W_hh = init_weights((self.out_size, self.out_size), prefix + \"W_hh\" + layer_id, sample = \"ortho\")\n",
        "        self.b_h = init_bias(self.out_size, prefix + \"b_h\" + layer_id)\n",
        "\n",
        "        X_4rz = T.dot(X, self.W_x_rz) + self.b_rz\n",
        "        X_4h = T.dot(X, self.W_xh) + self.b_h\n",
        "\n",
        "        def _slice(_x, n, dim):\n",
        "            if _x.ndim == 3:\n",
        "                return _x[:, :, n * dim : (n + 1) * dim]\n",
        "            return _x[:, n * dim : (n + 1) * dim]\n",
        "\n",
        "        def _active(m, x_4rz, x_4h, pre_h, W_h_rz, W_hh):\n",
        "            rz_preact = x_4rz + T.dot(pre_h, W_h_rz)\n",
        "            r = T.nnet.sigmoid(_slice(rz_preact, 0, self.out_size))\n",
        "            z = T.nnet.sigmoid(_slice(rz_preact, 1, self.out_size))\n",
        "            gh = T.tanh(x_4h + T.dot(r * pre_h, W_hh))\n",
        "            h = (1 - z) * pre_h + z * gh\n",
        "            h = h * m[:, None]\n",
        "            return h\n",
        "        \n",
        "        outputs, updates = theano.scan(_active,\n",
        "                                       sequences = [mask, X_4rz, X_4h],\n",
        "                                       outputs_info = [T.alloc(floatX(0.), batch_size, self.out_size)],\n",
        "                                       non_sequences = [self.W_h_rz, self.W_hh],\n",
        "                                       strict = True)\n",
        "        h = outputs\n",
        "        # dropout\n",
        "        if p > 0:\n",
        "            srng = T.shared_randomstreams.RandomStreams(rng.randint(999999))\n",
        "            drop_mask = srng.binomial(n = 1, p = 1-p, size = h.shape, dtype = theano.config.floatX)\n",
        "            self.activation = T.switch(T.eq(is_train, 1), h * drop_mask, h * (1 - p))\n",
        "        else:\n",
        "            self.activation = T.switch(T.eq(is_train, 1), h, h)\n",
        "       \n",
        "        self.params = [self.W_x_rz, self.W_h_rz, self.b_rz,\n",
        "                       self.W_xh, self.W_hh, self.b_h]\n",
        "\n",
        "class BdGRU(object):\n",
        "    # Bidirectional GRU Layer.\n",
        "    def __init__(self, rng, layer_id, shape, X, mask, is_train = 1, batch_size = 1, p = 0.5):\n",
        "        fwd = GRULayer(rng, \"_fwd_\" + layer_id, shape, X, mask, is_train, batch_size, p)\n",
        "        bwd = GRULayer(rng, \"_bwd_\" + layer_id, shape, X[::-1], mask[::-1], is_train, batch_size, p)\n",
        "        self.params = fwd.params + bwd.params\n",
        "        self.activation = T.concatenate([fwd.activation, bwd.activation[::-1]], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxgw9XI58eEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}